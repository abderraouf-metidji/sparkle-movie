{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexte du Projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place de PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\qwerty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\qwerty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installation de pyspark\n",
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT : PENSEZ A INSTALLER JAVA - winget install --exact --id EclipseAdoptium.Temurin.11.JDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix pour python 3.12 qui ne contient plus la librairie distutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    import distutils\n",
    "except ModuleNotFoundError:\n",
    "    import setuptools._distutils as distutils\n",
    "    sys.modules[\"distutils\"] = distutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MoviesRatings\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import et traitement des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des datasets ratings.csv & movies.csv\n",
    "ratings = spark.read.csv(\"ml-latest-small/ratings.csv\", header=True, inferSchema=True)\n",
    "movies = spark.read.csv(\"ml-latest-small/movies.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "|     1|     70|   3.0|964982400|\n",
      "|     1|    101|   5.0|964980868|\n",
      "|     1|    110|   4.0|964982176|\n",
      "|     1|    151|   5.0|964984041|\n",
      "|     1|    157|   5.0|964984100|\n",
      "+------+-------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On Affiche les les 10 premières lignes des deux datasets\n",
    "ratings.show(10)\n",
    "movies.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On affiche le schéma des deux datasets\n",
    "ratings.printSchema()\n",
    "movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes dans ratings :  100836\n",
      "Nombre de lignes dans movies :  9742\n"
     ]
    }
   ],
   "source": [
    "# On regarde le nombre de lignes de chaque dataset\n",
    "print(\"Nombre de lignes dans ratings : \", ratings.count())\n",
    "print(\"Nombre de lignes dans movies : \", movies.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col, avg, lit, explode, split, row_number\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     0|      0|     0|        0|\n",
      "+------+-------+------+---------+\n",
      "\n",
      "+-------+-----+------+\n",
      "|movieId|title|genres|\n",
      "+-------+-----+------+\n",
      "|      0|    0|     0|\n",
      "+-------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On regarde le nombre de valeurs manquantes dans chaque dataset\n",
    "\n",
    "ratings.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in ratings.columns]).show()\n",
    "movies.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in movies.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|100836| 100836|100836|   100836|\n",
      "+------+-------+------+---------+\n",
      "\n",
      "+-------+-----+------+\n",
      "|movieId|title|genres|\n",
      "+-------+-----+------+\n",
      "|   9742| 9742|  9742|\n",
      "+-------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On regarde le nombre de valeurs distinctes dans chaque dataset\n",
    "ratings.agg(*(count(col(c)).alias(c) for c in ratings.columns)).show()\n",
    "movies.agg(*(count(col(c)).alias(c) for c in movies.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+\n",
      "|summary|            rating|           timestamp|\n",
      "+-------+------------------+--------------------+\n",
      "|  count|            100836|              100836|\n",
      "|   mean| 3.501556983616962|1.2059460873684695E9|\n",
      "| stddev|1.0425292390606342|2.1626103599513078E8|\n",
      "|    min|               0.5|           828124615|\n",
      "|    max|               5.0|          1537799250|\n",
      "+-------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On regarde les statistiques descriptives des colonnes rating et timestamp du dataset ratings\n",
    "ratings.describe([\"rating\", \"timestamp\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|movieId|avg_rating|\n",
      "+-------+----------+\n",
      "|  26350|       5.0|\n",
      "|   3795|       5.0|\n",
      "|  25887|       5.0|\n",
      "| 157775|       5.0|\n",
      "|    633|       5.0|\n",
      "|  33138|       5.0|\n",
      "|  67618|       5.0|\n",
      "|    876|       5.0|\n",
      "|    496|       5.0|\n",
      "|  27373|       5.0|\n",
      "| 113829|       5.0|\n",
      "|  53578|       5.0|\n",
      "| 152711|       5.0|\n",
      "| 118894|       5.0|\n",
      "|     53|       5.0|\n",
      "| 160644|       5.0|\n",
      "|    148|       5.0|\n",
      "|   8911|       5.0|\n",
      "| 147300|       5.0|\n",
      "|  84273|       5.0|\n",
      "+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quels sont les films les mieux notés en moyenne \n",
    "\n",
    "ratings.groupBy(\"movieId\").agg(avg(\"rating\").alias(\"avg_rating\")).orderBy(\"avg_rating\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|movieId|count|\n",
      "+-------+-----+\n",
      "|    356|  329|\n",
      "|    318|  317|\n",
      "|    296|  307|\n",
      "|    593|  279|\n",
      "|   2571|  278|\n",
      "|    260|  251|\n",
      "|    480|  238|\n",
      "|    110|  237|\n",
      "|    589|  224|\n",
      "|    527|  220|\n",
      "|   2959|  218|\n",
      "|      1|  215|\n",
      "|   1196|  211|\n",
      "|     50|  204|\n",
      "|   2858|  204|\n",
      "|     47|  203|\n",
      "|    780|  202|\n",
      "|    150|  201|\n",
      "|   1198|  200|\n",
      "|   4993|  198|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quels sont les films les plus populaires \n",
    "ratings.groupBy(\"movieId\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----------------+------------+\n",
      "|title                                    |avg_rating       |count_rating|\n",
      "+-----------------------------------------+-----------------+------------+\n",
      "|Shawshank Redemption, The (1994)         |4.429022082018927|317         |\n",
      "|Godfather, The (1972)                    |4.2890625        |192         |\n",
      "|Fight Club (1999)                        |4.272935779816514|218         |\n",
      "|Godfather: Part II, The (1974)           |4.25968992248062 |129         |\n",
      "|Departed, The (2006)                     |4.252336448598131|107         |\n",
      "|Goodfellas (1990)                        |4.25             |126         |\n",
      "|Dark Knight, The (2008)                  |4.238255033557047|149         |\n",
      "|Usual Suspects, The (1995)               |4.237745098039215|204         |\n",
      "|Princess Bride, The (1987)               |4.232394366197183|142         |\n",
      "|Star Wars: Episode IV - A New Hope (1977)|4.231075697211155|251         |\n",
      "+-----------------------------------------+-----------------+------------+\n",
      "\n",
      "+-----------------------------------------------------+------------------+------------+\n",
      "|title                                                |avg_rating        |count_rating|\n",
      "+-----------------------------------------------------+------------------+------------+\n",
      "|Shawshank Redemption, The (1994)                     |4.429022082018927 |317         |\n",
      "|Fight Club (1999)                                    |4.272935779816514 |218         |\n",
      "|Usual Suspects, The (1995)                           |4.237745098039215 |204         |\n",
      "|Star Wars: Episode IV - A New Hope (1977)            |4.231075697211155 |251         |\n",
      "|Schindler's List (1993)                              |4.225             |220         |\n",
      "|Star Wars: Episode V - The Empire Strikes Back (1980)|4.2156398104265405|211         |\n",
      "|Pulp Fiction (1994)                                  |4.197068403908795 |307         |\n",
      "|Matrix, The (1999)                                   |4.192446043165468 |278         |\n",
      "|Forrest Gump (1994)                                  |4.164133738601824 |329         |\n",
      "|Silence of the Lambs, The (1991)                     |4.161290322580645 |279         |\n",
      "+-----------------------------------------------------+------------------+------------+\n",
      "\n",
      "+--------------------------------+-----------------+------------+\n",
      "|title                           |avg_rating       |count_rating|\n",
      "+--------------------------------+-----------------+------------+\n",
      "|Shawshank Redemption, The (1994)|4.429022082018927|317         |\n",
      "|Pulp Fiction (1994)             |4.197068403908795|307         |\n",
      "|Forrest Gump (1994)             |4.164133738601824|329         |\n",
      "+--------------------------------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quels sont les 10 films les mieux notés en moyenne avec plus de 100/200 & 300 notes\n",
    "\n",
    "# Jointure commune entre ratings et movies\n",
    "ratings_with_titles = ratings.join(movies, on=\"movieId\", how=\"inner\")\n",
    "\n",
    "# Fonction pour obtenir les films mieux notés avec un seuil minimal de votes\n",
    "def top_movies_by_avg(threshold, limit=10):\n",
    "    return (\n",
    "        ratings_with_titles\n",
    "        .groupBy(\"movieId\", \"title\")\n",
    "        .agg(avg(\"rating\").alias(\"avg_rating\"), count(\"rating\").alias(\"count_rating\"))\n",
    "        .filter(col(\"count_rating\") > threshold)\n",
    "        .orderBy(col(\"avg_rating\").desc())\n",
    "        .select(\"title\", \"avg_rating\", \"count_rating\")\n",
    "        .limit(limit)\n",
    "    )\n",
    "\n",
    "top_movies_by_avg(100).show(10, False)\n",
    "top_movies_by_avg(200).show(10, False)\n",
    "top_movies_by_avg(300).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Popular Movies:\n",
      "+-------+------------+------------------------------------------------------------------------------+-------------------------------------------+\n",
      "|movieId|count_rating|title                                                                         |genres                                     |\n",
      "+-------+------------+------------------------------------------------------------------------------+-------------------------------------------+\n",
      "|356    |329         |Forrest Gump (1994)                                                           |Comedy|Drama|Romance|War                   |\n",
      "|318    |317         |Shawshank Redemption, The (1994)                                              |Crime|Drama                                |\n",
      "|296    |307         |Pulp Fiction (1994)                                                           |Comedy|Crime|Drama|Thriller                |\n",
      "|593    |279         |Silence of the Lambs, The (1991)                                              |Crime|Horror|Thriller                      |\n",
      "|2571   |278         |Matrix, The (1999)                                                            |Action|Sci-Fi|Thriller                     |\n",
      "|260    |251         |Star Wars: Episode IV - A New Hope (1977)                                     |Action|Adventure|Sci-Fi                    |\n",
      "|480    |238         |Jurassic Park (1993)                                                          |Action|Adventure|Sci-Fi|Thriller           |\n",
      "|110    |237         |Braveheart (1995)                                                             |Action|Drama|War                           |\n",
      "|589    |224         |Terminator 2: Judgment Day (1991)                                             |Action|Sci-Fi                              |\n",
      "|527    |220         |Schindler's List (1993)                                                       |Drama|War                                  |\n",
      "|2959   |218         |Fight Club (1999)                                                             |Action|Crime|Drama|Thriller                |\n",
      "|1      |215         |Toy Story (1995)                                                              |Adventure|Animation|Children|Comedy|Fantasy|\n",
      "|1196   |211         |Star Wars: Episode V - The Empire Strikes Back (1980)                         |Action|Adventure|Sci-Fi                    |\n",
      "|50     |204         |Usual Suspects, The (1995)                                                    |Crime|Mystery|Thriller                     |\n",
      "|2858   |204         |American Beauty (1999)                                                        |Drama|Romance                              |\n",
      "|47     |203         |Seven (a.k.a. Se7en) (1995)                                                   |Mystery|Thriller                           |\n",
      "|780    |202         |Independence Day (a.k.a. ID4) (1996)                                          |Action|Adventure|Sci-Fi|Thriller           |\n",
      "|150    |201         |Apollo 13 (1995)                                                              |Adventure|Drama|IMAX                       |\n",
      "|1198   |200         |Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)|Action|Adventure                           |\n",
      "|4993   |198         |Lord of the Rings: The Fellowship of the Ring, The (2001)                     |Adventure|Fantasy                          |\n",
      "|1210   |196         |Star Wars: Episode VI - Return of the Jedi (1983)                             |Action|Adventure|Sci-Fi                    |\n",
      "|858    |192         |Godfather, The (1972)                                                         |Crime|Drama                                |\n",
      "|457    |190         |Fugitive, The (1993)                                                          |Thriller                                   |\n",
      "|592    |189         |Batman (1989)                                                                 |Action|Crime|Thriller                      |\n",
      "|5952   |188         |Lord of the Rings: The Two Towers, The (2002)                                 |Adventure|Fantasy                          |\n",
      "|2028   |188         |Saving Private Ryan (1998)                                                    |Action|Drama|War                           |\n",
      "|7153   |185         |Lord of the Rings: The Return of the King, The (2003)                         |Action|Adventure|Drama|Fantasy             |\n",
      "|588    |183         |Aladdin (1992)                                                                |Adventure|Animation|Children|Comedy|Musical|\n",
      "|608    |181         |Fargo (1996)                                                                  |Comedy|Crime|Drama|Thriller                |\n",
      "|2762   |179         |Sixth Sense, The (1999)                                                       |Drama|Horror|Mystery                       |\n",
      "+-------+------------+------------------------------------------------------------------------------+-------------------------------------------+\n",
      "\n",
      "\n",
      "Top 5 Movies by Genre:\n",
      "Genre: (no genres listed)\n",
      "+-------------------------------------------------------+------------------+------------+------------------+----+\n",
      "|title                                                  |genre             |count_rating|avg_rating        |rank|\n",
      "+-------------------------------------------------------+------------------+------------+------------------+----+\n",
      "|Pirates of the Caribbean: Dead Men Tell No Tales (2017)|(no genres listed)|7           |3.7857142857142856|1   |\n",
      "|Green Room (2015)                                      |(no genres listed)|3           |3.3333333333333335|2   |\n",
      "|Whiplash (2013)                                        |(no genres listed)|2           |4.75              |3   |\n",
      "|The Godfather Trilogy: 1972-1990 (1992)                |(no genres listed)|2           |4.75              |4   |\n",
      "|The Brand New Testament (2015)                         |(no genres listed)|2           |4.0               |5   |\n",
      "+-------------------------------------------------------+------------------+------------+------------------+----+\n",
      "\n",
      "Genre: Action\n",
      "+-----------------------------------------+------+------------+-----------------+----+\n",
      "|title                                    |genre |count_rating|avg_rating       |rank|\n",
      "+-----------------------------------------+------+------------+-----------------+----+\n",
      "|Matrix, The (1999)                       |Action|278         |4.192446043165468|1   |\n",
      "|Star Wars: Episode IV - A New Hope (1977)|Action|251         |4.231075697211155|2   |\n",
      "|Jurassic Park (1993)                     |Action|238         |3.75             |3   |\n",
      "|Braveheart (1995)                        |Action|237         |4.031645569620253|4   |\n",
      "|Terminator 2: Judgment Day (1991)        |Action|224         |3.970982142857143|5   |\n",
      "+-----------------------------------------+------+------------+-----------------+----+\n",
      "\n",
      "Genre: Adventure\n",
      "+-----------------------------------------------------+---------+------------+------------------+----+\n",
      "|title                                                |genre    |count_rating|avg_rating        |rank|\n",
      "+-----------------------------------------------------+---------+------------+------------------+----+\n",
      "|Star Wars: Episode IV - A New Hope (1977)            |Adventure|251         |4.231075697211155 |1   |\n",
      "|Jurassic Park (1993)                                 |Adventure|238         |3.75              |2   |\n",
      "|Toy Story (1995)                                     |Adventure|215         |3.9209302325581397|3   |\n",
      "|Star Wars: Episode V - The Empire Strikes Back (1980)|Adventure|211         |4.2156398104265405|4   |\n",
      "|Independence Day (a.k.a. ID4) (1996)                 |Adventure|202         |3.4455445544554455|5   |\n",
      "+-----------------------------------------------------+---------+------------+------------------+----+\n",
      "\n",
      "Genre: Animation\n",
      "+---------------------------+---------+------------+------------------+----+\n",
      "|title                      |genre    |count_rating|avg_rating        |rank|\n",
      "+---------------------------+---------+------------+------------------+----+\n",
      "|Toy Story (1995)           |Animation|215         |3.9209302325581397|1   |\n",
      "|Aladdin (1992)             |Animation|183         |3.7923497267759565|2   |\n",
      "|Lion King, The (1994)      |Animation|172         |3.941860465116279 |3   |\n",
      "|Shrek (2001)               |Animation|170         |3.8676470588235294|4   |\n",
      "|Beauty and the Beast (1991)|Animation|146         |3.7705479452054793|5   |\n",
      "+---------------------------+---------+------------+------------------+----+\n",
      "\n",
      "Genre: Children\n",
      "+---------------------------+--------+------------+------------------+----+\n",
      "|title                      |genre   |count_rating|avg_rating        |rank|\n",
      "+---------------------------+--------+------------+------------------+----+\n",
      "|Toy Story (1995)           |Children|215         |3.9209302325581397|1   |\n",
      "|Aladdin (1992)             |Children|183         |3.7923497267759565|2   |\n",
      "|Lion King, The (1994)      |Children|172         |3.941860465116279 |3   |\n",
      "|Shrek (2001)               |Children|170         |3.8676470588235294|4   |\n",
      "|Beauty and the Beast (1991)|Children|146         |3.7705479452054793|5   |\n",
      "+---------------------------+--------+------------+------------------+----+\n",
      "\n",
      "Genre: Comedy\n",
      "+-------------------+------+------------+------------------+----+\n",
      "|title              |genre |count_rating|avg_rating        |rank|\n",
      "+-------------------+------+------------+------------------+----+\n",
      "|Forrest Gump (1994)|Comedy|329         |4.164133738601824 |1   |\n",
      "|Pulp Fiction (1994)|Comedy|307         |4.197068403908795 |2   |\n",
      "|Toy Story (1995)   |Comedy|215         |3.9209302325581397|3   |\n",
      "|Aladdin (1992)     |Comedy|183         |3.7923497267759565|4   |\n",
      "|Fargo (1996)       |Comedy|181         |4.116022099447513 |5   |\n",
      "+-------------------+------+------------+------------------+----+\n",
      "\n",
      "Genre: Crime\n",
      "+--------------------------------+-----+------------+-----------------+----+\n",
      "|title                           |genre|count_rating|avg_rating       |rank|\n",
      "+--------------------------------+-----+------------+-----------------+----+\n",
      "|Shawshank Redemption, The (1994)|Crime|317         |4.429022082018927|1   |\n",
      "|Pulp Fiction (1994)             |Crime|307         |4.197068403908795|2   |\n",
      "|Silence of the Lambs, The (1991)|Crime|279         |4.161290322580645|3   |\n",
      "|Fight Club (1999)               |Crime|218         |4.272935779816514|4   |\n",
      "|Usual Suspects, The (1995)      |Crime|204         |4.237745098039215|5   |\n",
      "+--------------------------------+-----+------------+-----------------+----+\n",
      "\n",
      "Genre: Documentary\n",
      "+----------------------------+-----------+------------+------------------+----+\n",
      "|title                       |genre      |count_rating|avg_rating        |rank|\n",
      "+----------------------------+-----------+------------+------------------+----+\n",
      "|Bowling for Columbine (2002)|Documentary|58          |3.7758620689655173|1   |\n",
      "|Super Size Me (2004)        |Documentary|50          |3.51              |2   |\n",
      "|Fahrenheit 9/11 (2004)      |Documentary|37          |3.4864864864864864|3   |\n",
      "|Roger & Me (1989)           |Documentary|31          |3.838709677419355 |4   |\n",
      "|Hoop Dreams (1994)          |Documentary|29          |4.293103448275862 |5   |\n",
      "+----------------------------+-----------+------------+------------------+----+\n",
      "\n",
      "Genre: Drama\n",
      "+--------------------------------+-----+------------+-----------------+----+\n",
      "|title                           |genre|count_rating|avg_rating       |rank|\n",
      "+--------------------------------+-----+------------+-----------------+----+\n",
      "|Forrest Gump (1994)             |Drama|329         |4.164133738601824|1   |\n",
      "|Shawshank Redemption, The (1994)|Drama|317         |4.429022082018927|2   |\n",
      "|Pulp Fiction (1994)             |Drama|307         |4.197068403908795|3   |\n",
      "|Braveheart (1995)               |Drama|237         |4.031645569620253|4   |\n",
      "|Schindler's List (1993)         |Drama|220         |4.225            |5   |\n",
      "+--------------------------------+-----+------------+-----------------+----+\n",
      "\n",
      "Genre: Fantasy\n",
      "+---------------------------------------------------------+-------+------------+------------------+----+\n",
      "|title                                                    |genre  |count_rating|avg_rating        |rank|\n",
      "+---------------------------------------------------------+-------+------------+------------------+----+\n",
      "|Toy Story (1995)                                         |Fantasy|215         |3.9209302325581397|1   |\n",
      "|Lord of the Rings: The Fellowship of the Ring, The (2001)|Fantasy|198         |4.106060606060606 |2   |\n",
      "|Lord of the Rings: The Two Towers, The (2002)            |Fantasy|188         |4.0212765957446805|3   |\n",
      "|Lord of the Rings: The Return of the King, The (2003)    |Fantasy|185         |4.118918918918919 |4   |\n",
      "|Shrek (2001)                                             |Fantasy|170         |3.8676470588235294|5   |\n",
      "+---------------------------------------------------------+-------+------------+------------------+----+\n",
      "\n",
      "Genre: Film-Noir\n",
      "+------------------------+---------+------------+------------------+----+\n",
      "|title                   |genre    |count_rating|avg_rating        |rank|\n",
      "+------------------------+---------+------------+------------------+----+\n",
      "|L.A. Confidential (1997)|Film-Noir|97          |4.061855670103093 |1   |\n",
      "|Sin City (2005)         |Film-Noir|84          |3.857142857142857 |2   |\n",
      "|Chinatown (1974)        |Film-Noir|59          |4.211864406779661 |3   |\n",
      "|Mulholland Drive (2001) |Film-Noir|51          |3.843137254901961 |4   |\n",
      "|Dark City (1998)        |Film-Noir|48          |3.8229166666666665|5   |\n",
      "+------------------------+---------+------------+------------------+----+\n",
      "\n",
      "Genre: Horror\n",
      "+--------------------------------+------+------------+------------------+----+\n",
      "|title                           |genre |count_rating|avg_rating        |rank|\n",
      "+--------------------------------+------+------------+------------------+----+\n",
      "|Silence of the Lambs, The (1991)|Horror|279         |4.161290322580645 |1   |\n",
      "|Sixth Sense, The (1999)         |Horror|179         |3.893854748603352 |2   |\n",
      "|Alien (1979)                    |Horror|146         |3.969178082191781 |3   |\n",
      "|Aliens (1986)                   |Horror|126         |3.9642857142857144|4   |\n",
      "|Shining, The (1980)             |Horror|109         |4.08256880733945  |5   |\n",
      "+--------------------------------+------+------------+------------------+----+\n",
      "\n",
      "Genre: IMAX\n",
      "+---------------------------+-----+------------+------------------+----+\n",
      "|title                      |genre|count_rating|avg_rating        |rank|\n",
      "+---------------------------+-----+------------+------------------+----+\n",
      "|Apollo 13 (1995)           |IMAX |201         |3.845771144278607 |1   |\n",
      "|Lion King, The (1994)      |IMAX |172         |3.941860465116279 |2   |\n",
      "|Dark Knight, The (2008)    |IMAX |149         |4.238255033557047 |3   |\n",
      "|Beauty and the Beast (1991)|IMAX |146         |3.7705479452054793|4   |\n",
      "|Inception (2010)           |IMAX |143         |4.066433566433567 |5   |\n",
      "+---------------------------+-----+------------+------------------+----+\n",
      "\n",
      "Genre: Musical\n",
      "+------------------------------------------+-------+------------+------------------+----+\n",
      "|title                                     |genre  |count_rating|avg_rating        |rank|\n",
      "+------------------------------------------+-------+------------+------------------+----+\n",
      "|Aladdin (1992)                            |Musical|183         |3.7923497267759565|1   |\n",
      "|Lion King, The (1994)                     |Musical|172         |3.941860465116279 |2   |\n",
      "|Beauty and the Beast (1991)               |Musical|146         |3.7705479452054793|3   |\n",
      "|Willy Wonka & the Chocolate Factory (1971)|Musical|119         |3.8739495798319328|4   |\n",
      "|Nightmare Before Christmas, The (1993)    |Musical|93          |3.553763440860215 |5   |\n",
      "+------------------------------------------+-------+------------+------------------+----+\n",
      "\n",
      "Genre: Mystery\n",
      "+-----------------------------------------+-------+------------+------------------+----+\n",
      "|title                                    |genre  |count_rating|avg_rating        |rank|\n",
      "+-----------------------------------------+-------+------------+------------------+----+\n",
      "|Usual Suspects, The (1995)               |Mystery|204         |4.237745098039215 |1   |\n",
      "|Seven (a.k.a. Se7en) (1995)              |Mystery|203         |3.9753694581280787|2   |\n",
      "|Sixth Sense, The (1999)                  |Mystery|179         |3.893854748603352 |3   |\n",
      "|Twelve Monkeys (a.k.a. 12 Monkeys) (1995)|Mystery|177         |3.983050847457627 |4   |\n",
      "|Mission: Impossible (1996)               |Mystery|162         |3.537037037037037 |5   |\n",
      "+-----------------------------------------+-------+------------+------------------+----+\n",
      "\n",
      "Genre: Romance\n",
      "+----------------------+-------+------------+------------------+----+\n",
      "|title                 |genre  |count_rating|avg_rating        |rank|\n",
      "+----------------------+-------+------------+------------------+----+\n",
      "|Forrest Gump (1994)   |Romance|329         |4.164133738601824 |1   |\n",
      "|American Beauty (1999)|Romance|204         |4.056372549019608 |2   |\n",
      "|True Lies (1994)      |Romance|178         |3.497191011235955 |3   |\n",
      "|Speed (1994)          |Romance|171         |3.5292397660818713|4   |\n",
      "|Shrek (2001)          |Romance|170         |3.8676470588235294|5   |\n",
      "+----------------------+-------+------------+------------------+----+\n",
      "\n",
      "Genre: Sci-Fi\n",
      "+-----------------------------------------------------+------+------------+------------------+----+\n",
      "|title                                                |genre |count_rating|avg_rating        |rank|\n",
      "+-----------------------------------------------------+------+------------+------------------+----+\n",
      "|Matrix, The (1999)                                   |Sci-Fi|278         |4.192446043165468 |1   |\n",
      "|Star Wars: Episode IV - A New Hope (1977)            |Sci-Fi|251         |4.231075697211155 |2   |\n",
      "|Jurassic Park (1993)                                 |Sci-Fi|238         |3.75              |3   |\n",
      "|Terminator 2: Judgment Day (1991)                    |Sci-Fi|224         |3.970982142857143 |4   |\n",
      "|Star Wars: Episode V - The Empire Strikes Back (1980)|Sci-Fi|211         |4.2156398104265405|5   |\n",
      "+-----------------------------------------------------+------+------------+------------------+----+\n",
      "\n",
      "Genre: Thriller\n",
      "+--------------------------------+--------+------------+-----------------+----+\n",
      "|title                           |genre   |count_rating|avg_rating       |rank|\n",
      "+--------------------------------+--------+------------+-----------------+----+\n",
      "|Pulp Fiction (1994)             |Thriller|307         |4.197068403908795|1   |\n",
      "|Silence of the Lambs, The (1991)|Thriller|279         |4.161290322580645|2   |\n",
      "|Matrix, The (1999)              |Thriller|278         |4.192446043165468|3   |\n",
      "|Jurassic Park (1993)            |Thriller|238         |3.75             |4   |\n",
      "|Fight Club (1999)               |Thriller|218         |4.272935779816514|5   |\n",
      "+--------------------------------+--------+------------+-----------------+----+\n",
      "\n",
      "Genre: War\n",
      "+--------------------------+-----+------------+------------------+----+\n",
      "|title                     |genre|count_rating|avg_rating        |rank|\n",
      "+--------------------------+-----+------------+------------------+----+\n",
      "|Forrest Gump (1994)       |War  |329         |4.164133738601824 |1   |\n",
      "|Braveheart (1995)         |War  |237         |4.031645569620253 |2   |\n",
      "|Schindler's List (1993)   |War  |220         |4.225             |3   |\n",
      "|Saving Private Ryan (1998)|War  |188         |4.1462765957446805|4   |\n",
      "|Apocalypse Now (1979)     |War  |107         |4.219626168224299 |5   |\n",
      "+--------------------------+-----+------------+------------------+----+\n",
      "\n",
      "Genre: Western\n",
      "+-------------------------------------------------------------------------+-------+------------+------------------+----+\n",
      "|title                                                                    |genre  |count_rating|avg_rating        |rank|\n",
      "+-------------------------------------------------------------------------+-------+------------+------------------+----+\n",
      "|Dances with Wolves (1990)                                                |Western|164         |3.8353658536585367|1   |\n",
      "|Back to the Future Part III (1990)                                       |Western|88          |3.3693181818181817|2   |\n",
      "|Maverick (1994)                                                          |Western|74          |3.5               |3   |\n",
      "|Good, the Bad and the Ugly, The (Buono, il brutto, il cattivo, Il) (1966)|Western|72          |4.145833333333333 |4   |\n",
      "|Django Unchained (2012)                                                  |Western|71          |3.943661971830986 |5   |\n",
      "+-------------------------------------------------------------------------+-------+------------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quels genres de films sont les plus populaires ? (nb de notes) & Top 5 des films les plus populaires par genre\n",
    "\n",
    "def most_popular_movies(limit=30):\n",
    "    return (\n",
    "        ratings.groupBy(\"movieId\")\n",
    "               .agg(count(\"rating\").alias(\"count_rating\"))\n",
    "               .join(movies, \"movieId\")\n",
    "               .orderBy(col(\"count_rating\").desc())\n",
    "               .limit(limit)\n",
    "    )\n",
    "\n",
    "def top5_by_genre():\n",
    "    movies_with_genre = movies.withColumn(\"genre\", explode(split(col(\"genres\"), \"\\\\|\")))\n",
    "    movies_ratings = ratings.join(movies_with_genre, \"movieId\")\n",
    "    movies_stats = movies_ratings.groupBy(\"title\", \"genre\") \\\n",
    "        .agg(count(\"rating\").alias(\"count_rating\"), avg(\"rating\").alias(\"avg_rating\"))\n",
    "    windowSpec = Window.partitionBy(\"genre\").orderBy(col(\"count_rating\").desc())\n",
    "    return movies_stats.withColumn(\"rank\", row_number().over(windowSpec)) \\\n",
    "        .filter(col(\"rank\") <= 5) \\\n",
    "        .orderBy(\"genre\", \"rank\")\n",
    "\n",
    "def display_top5_by_genre(limit_per_genre=5):\n",
    "    df = top5_by_genre()\n",
    "    # Récupérer la liste des genres distincts\n",
    "    genres = [row[\"genre\"] for row in df.select(\"genre\").distinct().collect()]\n",
    "    for genre in sorted(genres):\n",
    "        print(f\"Genre: {genre}\")\n",
    "        df.filter(col(\"genre\") == genre).orderBy(\"rank\").show(limit_per_genre, False)\n",
    "\n",
    "# Utilisation :\n",
    "\n",
    "print(\"Most Popular Movies:\")\n",
    "most_popular_movies().show(30, False)\n",
    "\n",
    "print(\"\\nTop 5 Movies by Genre:\")\n",
    "display_top5_by_genre()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelisation avec Spark MLlib : ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S'assurer que les colonnes ont le bon type\n",
    "ratings = ratings.withColumn(\"userId\", col(\"userId\").cast(\"integer\")) \\\n",
    "                 .withColumn(\"movieId\", col(\"movieId\").cast(\"integer\")) \\\n",
    "                 .withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "\n",
    "# Diviser en 80% training et 20% test\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(\n",
    "    maxIter=15,           # Nombre d'itérations\n",
    "    regParam=0.095,         # Paramètre de régularisation\n",
    "    rank=10,              # Dimension des facteurs latents\n",
    "    userCol=\"userId\", \n",
    "    itemCol=\"movieId\", \n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\"  # Pour ignorer les prédictions NaN lors de l'évaluation\n",
    ")\n",
    "\n",
    "# Entraîner le modèle sur le jeu de données d'entraînement\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.8852306402685002\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Générer des prédictions sur le test set\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Configurer l'évaluateur RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", \n",
    "    labelCol=\"rating\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = {}\".format(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error = 0.681785940383638\n",
      "R² = 0.26742053446909564\n"
     ]
    }
   ],
   "source": [
    "# Utilisation d'autres métriques d'évaluation\n",
    "evaluator.setMetricName(\"mae\")\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print(\"Mean absolute error = {}\".format(mae))\n",
    "\n",
    "evaluator.setMetricName(\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(\"R² = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                                                                                                                        |\n",
      "+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1     |[{3379, 5.5808444}, {26258, 5.5283027}, {3836, 5.432815}, {188751, 5.4045467}, {174909, 5.4045467}, {3429, 5.3752484}, {6442, 5.3601003}, {183897, 5.359135}, {98491, 5.35116}, {69524, 5.3501325}]    |\n",
      "|2     |[{131724, 4.90717}, {26258, 4.725958}, {1379, 4.6923}, {55363, 4.6667967}, {59814, 4.6502147}, {85342, 4.6366262}, {7981, 4.6275215}, {26810, 4.6033354}, {55908, 4.597806}, {123, 4.55803}]           |\n",
      "|3     |[{4821, 5.5958157}, {70946, 5.076976}, {6835, 4.9288497}, {5746, 4.9288497}, {5919, 4.8837676}, {5181, 4.877048}, {2851, 4.8545485}, {33834, 4.838003}, {4518, 4.7776237}, {93563, 4.532418}]          |\n",
      "|4     |[{1293, 5.4543056}, {132333, 5.284213}, {5522, 5.231366}, {5915, 5.124128}, {6375, 5.0740533}, {8542, 5.0321946}, {7247, 5.0298676}, {1282, 5.0216045}, {55276, 5.0145936}, {246, 4.954978}]           |\n",
      "|5     |[{926, 5.03438}, {7700, 4.9400463}, {1262, 4.8870068}, {3089, 4.7573447}, {1250, 4.748188}, {1283, 4.653963}, {25825, 4.644314}, {2843, 4.6087103}, {908, 4.5947747}, {720, 4.5918007}]                |\n",
      "|6     |[{1046, 5.0115113}, {932, 4.9102197}, {55363, 4.815283}, {611, 4.801627}, {42730, 4.7402906}, {5867, 4.7402906}, {26528, 4.73846}, {159093, 4.725147}, {2843, 4.704127}, {51931, 4.6793356}]           |\n",
      "|7     |[{3200, 5.1055713}, {4967, 5.014555}, {1178, 4.952606}, {1232, 4.9080067}, {417, 4.8955274}, {2423, 4.886295}, {26258, 4.8720756}, {55276, 4.8651257}, {246, 4.858433}, {3379, 4.8369045}]             |\n",
      "|8     |[{85774, 4.7056828}, {158872, 4.6981945}, {184245, 4.697357}, {179135, 4.697357}, {171495, 4.697357}, {138966, 4.697357}, {134796, 4.697357}, {117531, 4.697357}, {86237, 4.697357}, {74226, 4.697357}]|\n",
      "|9     |[{5222, 5.4075437}, {720, 4.8259726}, {134853, 4.7969875}, {322, 4.702174}, {92535, 4.6958146}, {102217, 4.6580095}, {92494, 4.6580095}, {33779, 4.6580095}, {77800, 4.6083355}, {6159, 4.604713}]     |\n",
      "|10    |[{2843, 4.9345207}, {5066, 4.924864}, {71579, 4.8171735}, {8869, 4.7798123}, {26614, 4.75026}, {3086, 4.6872854}, {7121, 4.682103}, {140110, 4.616325}, {93008, 4.60412}, {77846, 4.60412}]            |\n",
      "+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Générer des recommandations pour tous les utilisateurs\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "userRecs.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------+---------+\n",
      "|userId|title                           |rating   |\n",
      "+------+--------------------------------+---------+\n",
      "|123   |Imposter, The (2012)            |4.8326015|\n",
      "|123   |De platte jungle (1978)         |4.7721395|\n",
      "|123   |Blue Planet II (2017)           |4.7721395|\n",
      "|123   |Cosmos                          |4.7721395|\n",
      "|123   |Nasu: Summer in Andalusia (2003)|4.7721395|\n",
      "+------+--------------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Générer les recommandations pour un utilisateur spécifique\n",
    "userRecs = model.recommendForUserSubset(ratings.filter(col(\"userId\") == 123), 5)\n",
    "userRecs = userRecs.withColumn(\"rec\", explode(col(\"recommendations\"))) \\\n",
    "                   .select(col(\"userId\"), col(\"rec.movieId\"), col(\"rec.rating\")) \\\n",
    "                   .join(movies, \"movieId\", \"left\") \\\n",
    "                   .select(\"userId\", \"title\", \"rating\")\n",
    "\n",
    "userRecs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommandation basée sur le contenu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommandation basée sur les proximités utilisateurs (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1532.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1440.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1440.0 (TID 1706) (DESKTOP-R9GVEP8 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m\"\u001b[39m, split_genres_udf(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Convertir le DataFrame PySpark en DataFrame Pandas\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m df_pandas \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Utilisation de MultiLabelBinarizer de sklearn pour encoder les genres\u001b[39;00m\n\u001b[0;32m     27\u001b[0m mlb \u001b[38;5;241m=\u001b[39m MultiLabelBinarizer()\n",
      "File \u001b[1;32mc:\\Users\\QWERTY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[0;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\QWERTY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\QWERTY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\QWERTY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\QWERTY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1532.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1440.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1440.0 (TID 1706) (DESKTOP-R9GVEP8 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Initialisation de Spark\n",
    "spark = SparkSession.builder.appName(\"MovieRecommendation\").getOrCreate()\n",
    "\n",
    "# Charger les données\n",
    "df = spark.read.csv(\"ml-latest-small/movies.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Transformer les genres en liste de genres\n",
    "def split_genres(genres_str):\n",
    "    return genres_str.split('|') if genres_str else []\n",
    "\n",
    "# Enregistrer la fonction en tant que UDF pour PySpark\n",
    "split_genres_udf = udf(split_genres, ArrayType(StringType()))\n",
    "df = df.withColumn(\"genres\", split_genres_udf(df[\"genres\"]))\n",
    "\n",
    "# Convertir le DataFrame PySpark en DataFrame Pandas\n",
    "df_pandas = df.select(\"title\", \"genres\").toPandas()\n",
    "\n",
    "# Utilisation de MultiLabelBinarizer de sklearn pour encoder les genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded = mlb.fit_transform(df_pandas[\"genres\"])\n",
    "X = pd.DataFrame(genres_encoded, columns=mlb.classes_)\n",
    "\n",
    "# Initialisation du modèle KNN de sklearn\n",
    "knn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "knn.fit(X)\n",
    "\n",
    "# Fonction de recommandation\n",
    "def recommend_movies(movie_title):\n",
    "    # Trouver l'index du film par titre\n",
    "    idx = df_pandas[df_pandas[\"title\"] == movie_title].index[0]\n",
    "    # Obtenir les indices des films les plus similaires\n",
    "    distances, indices = knn.kneighbors([X.iloc[idx]])\n",
    "    # Extraire les titres des films recommandés\n",
    "    recommendations = df_pandas.iloc[indices[0][1:]][\"title\"].values\n",
    "    return recommendations\n",
    "\n",
    "# Exemple de recommandation\n",
    "movie_example = \"Toy Story (1995)\"\n",
    "recommended_movies = recommend_movies(movie_example)\n",
    "print(f\"Films similaires à '{movie_example}': {recommended_movies}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des approches de recommandation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1573.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1445.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1445.0 (TID 1711) (DESKTOP-R9GVEP8 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m movies_df \u001b[38;5;241m=\u001b[39m movies_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m\"\u001b[39m, split_genres_udf(movies_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# ✅ Convertir en DataFrame Pandas\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m movies_pandas \u001b[38;5;241m=\u001b[39m \u001b[43mmovies_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovieId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m ratings_pandas \u001b[38;5;241m=\u001b[39m ratings_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovieId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# ✅ Vérification des données\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\QWERTY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[0;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\QWERTY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\QWERTY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\QWERTY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\QWERTY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1573.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1445.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1445.0 (TID 1711) (DESKTOP-R9GVEP8 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ✅ Initialisation de Spark\n",
    "spark = SparkSession.builder.appName(\"MovieRecommendation\").getOrCreate()\n",
    "\n",
    "# ✅ Charger les données\n",
    "movies_df = spark.read.csv(\"ml-latest-small/movies.csv\", header=True, inferSchema=True)\n",
    "ratings_df = spark.read.csv(\"ml-latest-small/ratings.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# ✅ Transformer les genres en liste\n",
    "def split_genres(genres_str):\n",
    "    return genres_str.split('|') if genres_str else []\n",
    "\n",
    "split_genres_udf = udf(split_genres, ArrayType(StringType()))\n",
    "movies_df = movies_df.withColumn(\"genres\", split_genres_udf(movies_df[\"genres\"]))\n",
    "\n",
    "# ✅ Convertir en DataFrame Pandas\n",
    "movies_pandas = movies_df.select(\"movieId\", \"title\", \"genres\").toPandas()\n",
    "ratings_pandas = ratings_df.select(\"userId\", \"movieId\", \"rating\").toPandas()\n",
    "\n",
    "# ✅ Vérification des données\n",
    "print(f\"Nombre total de films : {len(movies_pandas)}\")\n",
    "print(f\"Nombre total de notes : {len(ratings_pandas)}\")\n",
    "\n",
    "# ✅ Vérifier s’il y a des films bien notés\n",
    "high_ratings = ratings_pandas[ratings_pandas[\"rating\"] >= 4]\n",
    "print(f\"Nombre total de notes >= 4 : {len(high_ratings)}\")\n",
    "\n",
    "if high_ratings.empty:\n",
    "    print(\"Aucun film n'a été noté ≥ 4. Impossible d'évaluer la précision.\")\n",
    "    exit()\n",
    "\n",
    "# ✅ Encoder les genres avec MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded = mlb.fit_transform(movies_pandas[\"genres\"])\n",
    "X = pd.DataFrame(genres_encoded, columns=mlb.classes_)\n",
    "X.index = movies_pandas[\"movieId\"]\n",
    "\n",
    "# ✅ Initialisation du modèle KNN\n",
    "knn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "knn.fit(X)\n",
    "\n",
    "# ✅ Fonction de recommandation\n",
    "def recommend_movies(movie_id, num_recommendations=5):\n",
    "    if movie_id not in X.index:\n",
    "        return []\n",
    "    idx = X.index.get_loc(movie_id)\n",
    "    distances, indices = knn.kneighbors([X.iloc[idx]], n_neighbors=num_recommendations + 1)\n",
    "    recommended_ids = X.index[indices[0][1:]]  # Exclure le film de référence\n",
    "    return movies_pandas[movies_pandas[\"movieId\"].isin(recommended_ids)][\"title\"].tolist()\n",
    "\n",
    "# ✅ Évaluation de la Précision@K\n",
    "def precision_at_k(user_id, k=5):\n",
    "    user_ratings = ratings_pandas[ratings_pandas[\"userId\"] == user_id]\n",
    "    liked_movies = user_ratings[user_ratings[\"rating\"] >= 4][\"movieId\"].tolist()\n",
    "    \n",
    "    if not liked_movies:\n",
    "        return None  # Pas assez de données pour cet utilisateur\n",
    "    \n",
    "    recommended = []\n",
    "    for movie_id in liked_movies:\n",
    "        recommended.extend(recommend_movies(movie_id, num_recommendations=k))\n",
    "\n",
    "    recommended = list(set(recommended))  # Supprimer les doublons\n",
    "    hits = sum(1 for movie in recommended[:k] if movie in liked_movies)\n",
    "    return hits / k if recommended else 0\n",
    "\n",
    "# ✅ Évaluation de la Couverture des Recommandations\n",
    "def item_coverage():\n",
    "    recommended_movies = set()\n",
    "    for movie_id in X.index:\n",
    "        recommended_movies.update(recommend_movies(movie_id, num_recommendations=5))\n",
    "    \n",
    "    return len(recommended_movies) / len(X.index)\n",
    "\n",
    "# ✅ Sélection d'un utilisateur ayant noté des films ≥ 4\n",
    "users_with_ratings = high_ratings[\"userId\"].unique()\n",
    "user_test = np.random.choice(users_with_ratings)\n",
    "\n",
    "# ✅ Affichage des métriques\n",
    "precision = precision_at_k(user_test, k=5)\n",
    "coverage = item_coverage()\n",
    "\n",
    "print(f\"\\n📊 **Évaluation du modèle**\")\n",
    "print(f\"Utilisateur testé : {user_test}\")\n",
    "print(f\"Précision@5 pour l'utilisateur {user_test}: {precision:.2%}\" if precision is not None else \"Pas assez de films notés pour cet utilisateur.\")\n",
    "print(f\"Couverture des recommandations: {coverage:.2%}\")\n",
    "\n",
    "# ✅ Génération d'utilisateurs fictifs\n",
    "fake_users = {\n",
    "    \"Alice\": [\"Toy Story (1995)\", \"Finding Nemo (2003)\"],\n",
    "    \"Bob\": [\"The Matrix (1999)\", \"Inception (2010)\"],\n",
    "    \"Charlie\": [\"Titanic (1997)\", \"The Notebook (2004)\"],\n",
    "    \"David\": [\"The Godfather (1972)\", \"Goodfellas (1990)\"],\n",
    "    \"Eve\": [\"The Conjuring (2013)\", \"Get Out (2017)\"]\n",
    "}\n",
    "\n",
    "# ✅ Génération des recommandations pour chaque utilisateur fictif\n",
    "for user, liked_movies in fake_users.items():\n",
    "    recommended_movies = set()\n",
    "    \n",
    "    print(f\"\\n🎬 **Recommandations pour {user}** (Aime : {', '.join(liked_movies)})\")\n",
    "    \n",
    "    for movie in liked_movies:\n",
    "        # Trouver l'ID du film\n",
    "        movie_row = movies_pandas[movies_pandas[\"title\"] == movie]\n",
    "        if not movie_row.empty:\n",
    "            movie_id = movie_row[\"movieId\"].values[0]\n",
    "            recommended_movies.update(recommend_movies(movie_id, num_recommendations=3))\n",
    "\n",
    "    # Afficher les recommandations uniques\n",
    "    print(f\"📌 Suggestions : {', '.join(recommended_movies) if recommended_movies else 'Aucune recommandation trouvée'}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
